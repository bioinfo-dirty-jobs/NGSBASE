---
layout: page
title: Bioinformatics approac examples
summary: "We have many way to write our pipelines. Each approach have some problems. We need to use take in mind what we need to do."
---

![Pipeline]({{site.url}}/images/pipeline.jpg)

## Introduction

In Unix-like computer operating systems, a pipeline is a sequence of processes chained together by their standard streams, so that the output of each process (stdout) feeds directly as input (stdin) to the next one.

The concept of pipelines was championed by Douglas McIlroy at Unix's ancestral home of Bell Labs, during the development of Unix, shaping its toolbox philosophy.[1][2] It is named by analogy to a physical pipeline.

The standard shell syntax for pipelines is to list multiple commands, separated by vertical bars ("pipes" in common Unix verbiage). For example, to list files in the current directory (ls), retain only the lines of ls output containing the string "key" (grep), and view the result in a scrolling page (less), a user types the following into the command line of a terminal:

  ```ls -l | grep key | less```

"ls -l" produces a process, the output (stdout) of which is piped to the input (stdin) of the process for "grep key"; and likewise for the process for "less". Each process takes input from the previous process and produces output for the next process via standard streams. Each "|" tells the shell to connect the standard output of the command on the left to the standard input of the command on the right by an inter-process communication mechanism called an (anonymous) pipe, implemented in the operating system. Pipes are unidirectional; data flows through the pipeline from left to right.

``` process1 | process2 | process3 ```



In general a pipeline on bionformatics  refers to a prescribed set of processing steps needed to transfer form raw data into something interpretable. This could be for instance, a series of scripts or programs. This is an example of pipeeline for  germline calling process encode by GATK [GATK](https://software.broadinstitute.org/gatk/best-practices/)


        ![Pipeline]({{site.url}}/images/06.pipeline2.png)

The GATK Best Practices provide step-by-step recommendations for performing variant discovery analysis in high-throughput sequencing (HTS) data. There are several different GATK Best Practices workflows tailored to particular applications depending on the type of variation of interest and the technology employed. The Best Practices documentation attempts to describe in detail the key principles of the processing and analysis steps required to go from raw reads coming off the sequencing machine, all the way to an appropriately filtered variant callset that can be used in downstream analyses. Wherever we can, we try to provide guidance regarding experimental design, quality control (QC) and pipeline implementation options, but please understand that those are dependent on many factors including sequencing technology and the hardware infrastructure that are at your disposal, so you may need to adapt our recommendations to your specific situation.